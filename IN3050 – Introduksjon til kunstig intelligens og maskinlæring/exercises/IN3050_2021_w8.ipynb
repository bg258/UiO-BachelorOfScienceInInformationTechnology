{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08: Scaling and polynomial features\n",
    "### Introduction\n",
    "\n",
    "In this set 8, we will not introduce new learning algorithms, but instead consider two useful tools for machine learning:\n",
    "\n",
    "- Feature scaling\n",
    "- Feature engineering: Polynomial features\n",
    "\n",
    "To make it simple, we consider both of them in the context of (enhanced) linear regression. You should keep in mind, however, that\n",
    "\n",
    "- They are equally relevant to classification as to regression.\n",
    "- Feature scaling applies in similar ways to all learners using gradient descent.\n",
    "- Polynomial features may be added to any linear learners.\n",
    "\n",
    "To keep things even simpler, we will reuse a data set and some of the classes and methods we introduced in the exercise set week 07. We have therefore included the first half of the exercise set from week 07 here. What is new, comes from part B, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From week 7: Linear and logistic regression\n",
    "### Introduction\n",
    "\n",
    "This week, we will get some first-hand experience with regression.\n",
    "We will implement gradient descent for linear regression. Then we will proceed to classification, first by using linear regression and then logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "We will first familiarize ourselves a little with NumPy. A function which we will use over again is `linspace(x1,x2,N)` which makes a vector of length $N$ splitting the interval $[x1,x2]$ into equally sized intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-5,5,100)\n",
    "xx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major improvements from using NumPy is the possibilty of computing many values by applying a function to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = -6*xx**3 + xx**2 -3*xx + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y1`will contain the corresponding function values for each element `x`in `xx`. We may plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xx,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for linear regression\n",
    "We will start with a smooth curve and add some \"noise\". The underlying idea is that the smooth curve represents the function we are looking for, and that this is the best we can hope to learn. A solution which does better  on the training material than the smooth curve is probably overfitted and will not generalize as well to new data as the smooth curve. We are using a normal distribution to generate noise. The numpy function `normal` will generate a vector of `size` many random points around `loc` from a distribution with standard deviation `scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "t = y1 + normal(loc=0, scale=50, size=100)\n",
    "plt.scatter(xx, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data now consist of pairs (xx[i], t[i]), where xx[i] is the datapoint and t[i] the target value. So far, both `xx` and `t` are vectors. Check their shapes, e.g., `xx.shape`. The goal is to make an implementation for linear regression which works with an arbitrary number of input variables and not just one. We will therefore transform `xx` to a matrix of dimension $M\\times n$ where each row represents one datapoint, and n is the number of input variables (or features). Check the shape of `X` after the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xx.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Linear regression\n",
    "\n",
    "We will implement our own linear regression model. Our aim is to find an approximate function that fits the data generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with only one input variable, we start with a simple linear function, $f(x_1) = w_0 + w_1x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1: MSE\n",
    "\n",
    "We wonder if our $f$ fits the data well, and what parameters will give us the best approximation. We will estimate this using the Mean Squared Error:\n",
    "\n",
    "$\\frac{1}{N} \\sum_{j=1}^{N} (t_j - \\sum_{i=0}^{m} w_ix_{ji})^2$\n",
    "\n",
    "Write a function calculating MSE of our approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    sum_errors = 0.\n",
    "    for i in range(0,len(y)):\n",
    "        sum_errors += (y[i] - y_pred[i])**2\n",
    "    mean_squared_error = sum_errors/len(y)\n",
    "    return mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector form solution\n",
    "def mse_2(x,y):\n",
    "    return sum((x - y)**2) /x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: testing the MSE\n",
    "To test our implementation, we can take the function $f(x_1)=0$ as a baseline and calculate the MSE for this $f$. Also calculate the Root Means Square Error which provides a more natural measure for how good the fit is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = np.array([0 for x in xx])\n",
    "mse = mse_2(hypothesis, t)\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: \", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Adding bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement linear regression with gradient descent and test it on the data. To make it simple, we will add a $x_0=1$ to all our datapoints, and consider $f(x_1) = w_0 + w_1x_1$ as $f(x_0, x_1) = w_0x_0+ w_1x_1$. Make a procedure that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    sh = X.shape\n",
    "    m = sh[0]\n",
    "    bias = np.ones((m,1)) # Makes a m*1 matrix of 1-s\n",
    "    # Concatenate the column of bias in front of the columns of X.\n",
    "    return np.concatenate((bias, X), axis  = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end  of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exrcise 7.4: Gradient Descent\n",
    "We will implement the linear regression in a class as we did with the classifiers earlier. The fit method will run the gradient descent step a number of times to train the classifier. The predict method should take a matrix containing several data points and predict the outcome for all of them. Fill in the methods.\n",
    "\n",
    "Assume that the matrix of training data are not extended with bias features. Hence, make adding bias a part of your methods.\n",
    "\n",
    "After training there should be an attribute with learned coeffecients (weights) which is applied by the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinReg():\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"X is a Kxm matrix for some K>=1\n",
    "        predict the value for each point in X\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinReg():\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        (N, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.weights = weights = np.zeros(m+1)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            weights -= eta / N *  X_train.T @ (X_train @ weights - t_train)      \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"X is a Kxm matrix for some K>=1\n",
    "        predict the value for each point in X\"\"\"\n",
    "        Z = add_bias(X)\n",
    "        return Z @ self.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5: Train and test the model\n",
    "Fit the model to the training data. Report the coefficients. Plot the line together with the observations. Calculate the RMSE. Is the result a better fit than the baseline constant function $f(x)=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = NumpyLinReg()\n",
    "reg.fit(X,t, epochs=100)\n",
    "print(\"The coefficients: \", reg.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, reg.weights[0]+reg.weights[1]*X, color=\"r\")\n",
    "plt.plot(X, [0 for i in X], 'g')\n",
    "\n",
    "# The generated dataset\n",
    "plt.scatter(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mse_2(t, add_bias(X) @ reg.weights)\n",
    "print(\"MSE: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE :\", np.sqrt(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of week 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here starts the real set 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you find this part difficult, you may skip it and move to the rest of the exercise set which does not depend on you doing this part.)\n",
    "\n",
    "We mentioned in the lectures that the linear regression problem has a closed-form solution. Say that we have a linear regression problem where we try to predict $y$ from $x_1, x_2, \\ldots,x_n$ by a linear formula  $f(\\mathbf{x})=w_0+w_1x_1+\\cdots+w_nx_n$ on the basis of $N$ observations of the form $(\\mathbf{x}_i, t_i)$.\n",
    "If we extend each observation with a bias $x_0=1$ and put the $\\mathbf{x}_i$-s into a \n",
    "$(N\\times(n+1))$ matrix, and the $t_i$-s into a $N\\times 1$ column matrix $Y$, as we have done above, then we can find the weights (coeffecients/parameters) with the formula\n",
    "$W =\\theta = (X^T X)^{-1}X^TY$.\n",
    " \n",
    "Some explanation to the formula. A square matrix $A$ is called the *identity matrix* if $A[i,i]=1$ for all $i$, and $A[i,j]=0$ if $i\\neq j$. We use $I$ as a name of this matrix. It is called the *identity matrix* because $AI=IA=A$ for all $A$.\n",
    "\n",
    "Given a square matrix $A$. If there exists a matrix $B$, such that $AB=BA=I$, we will say that $B$ is the *inverse* of $A$ and write this $B=A^{-1}$. Not all matrices have inverses.\n",
    "\n",
    "In NumPy, the function `np.linalg.inv(X)` yields the inverse of `X` if it exists.\n",
    "\n",
    "We wil not try to explain why the formula $\\theta = (X^T X)^{-1}X^TY$ yields the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1: closed form solution\n",
    "Find the closed form solution to the dataset above and compare it to the solution you found by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Scaling the data\n",
    "\n",
    "In the lectures, we mentioned the importance of scaling/normalizing/standardizing the data for the *k*NN algorithm. We will here consider why scaling is important for gradient descent-based algorithms. Let's see the effects of this on a practical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "First, we'll load a dataset with features that intentionally vary in scale. \"ex1data2.txt\" (https://github.com/nex3z/machine-learning-exercise) is a dataset of housing prices, including the size of the house (in square feet), the number of bedrooms, and the price of the house in dollars.\n",
    "\n",
    "For the record, 100 sqm is about the same as 1100 sq. feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"ex1data2.txt\", \"r\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict the price from the size and the number of bedrooms, so we start by splitting the input data from the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data[:, 0:2]\n",
    "t2 = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2: Without scaling\n",
    "\n",
    "First, let's try to fit our NumpyLinReg to the data. Experiment with various training rates and number of epochs. Print the weights (coeffecients). Try to predict the price of a 3-bedroom 1500 sq. feet house. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3: Scaling\n",
    "\n",
    "The reason for our problems is the large difference in scale between our two input features -- 3 orders of magnitude. A good idea could be to scale the data before you start to experiment. Implement a max-min scaler and scale your data. By the way, it is only the input data `X2` which have to be scaled. It is normally not necessary to scale the output values `t2`/`y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4: With scaling\n",
    "Try to fit a linear regression to the scaled data. Experiment with various training rates and number of epochs. Print the weights (coeffecients). Try to predict the price of a 3-bedroom 1500 sq. feet house. How does this go compared to the experiments without scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Polynomial features\n",
    "We return to our first dataset, (X,t) from exercise set week07. We tried to fit a straight line to our dataset. From the way the set was generated, we think we would get a better fit if we try to predict a polynomial. We will now try to fit a polynomial. One way of achieving this is to extend our dataset with more features corresponding to the polynomial.\n",
    "\n",
    "We can add features for $x^2$, $x^3$, $x^4$, $x^5$, etc. to the data set. Since the resulting features will have different scales, after adding the polynomial features, we should scale all the features (except the bias, of course). Then we can try to fit a linear expression (a hyperplane) to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5\n",
    "Add features for $x^2$ and $x^3$. Scale the data. Fit a linear regression model to the data. Calculate the MSE. Compare to the MSE for the linear model. Plot the polynomial together with the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
